{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2heads_50batch_trueshuffle_dataset1.9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmfn098/transformer/blob/master/2heads_50batch_trueshuffle_dataset1_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmGviHsaK6F"
      },
      "source": [
        "# Machine Translation con Redes Transformer\n",
        "\n",
        "![Red Transformer](https://drive.google.com/uc?export=view&id=1nHAZ3WDsXYZuSPYximLx-hLa2S1zv9Cg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7-Shpflc88Z",
        "outputId": "1f6cca8b-60f4-49c7-a3ed-4b5f3248f077"
      },
      "source": [
        "pip install keras-transformer\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-transformer\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/35/6b079e920fe09a9349028bc2f209447e5636d90e29c5cf060bcc3177803a/keras-transformer-0.39.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-transformer) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-transformer) (2.4.3)\n",
            "Collecting keras-pos-embd>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/d2/1cc072ea68b573f366e08936177a33e237e66fa7d5338289d4bee64696cf/keras-pos-embd-0.12.0.tar.gz\n",
            "Collecting keras-multi-head>=0.28.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/e6/a83f26b2e1582de237b125f595874d808e40698f31d44d5903e872d5b64d/keras-multi-head-0.28.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.15.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/e1/0da586d544a0940a56a2f4aa704b7dbd95eaa8ceda6168b48f5ac95e6608/keras-layer-normalization-0.15.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/58/02/cd3e7e51cf45d3825818384a2f7d9c340b60c9bf55a5682b7318e1c16eab/keras-position-wise-feed-forward-0.7.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/48/78f6d134f1ede597d91186819c9e428ada51cd8d9ea28e5faf37ed2ee602/keras-embed-sim-0.9.0.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-transformer) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-transformer) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-transformer) (3.1.0)\n",
            "Collecting keras-self-attention>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/75/e6bc5b43ee968fef714f2f10a2a1674639ec85d2428cc47b2fe1f9af0115/keras-self-attention-0.50.0.tar.gz\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-transformer) (1.5.2)\n",
            "Building wheels for collected packages: keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.39.0-cp37-none-any.whl size=12841 sha256=2b8ef42faa35b201d4738c0dfdc98c47f3d399dc9ff7ab6867354da7afc9feb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/42/35/d33c5907bca04ac5742e9eceefb644b680286de26728506a70\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-cp37-none-any.whl size=7471 sha256=a270a420b316872d8003e24639b9c5ad42545cc0aaf16c36a6346189d98932ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/d8/36/06ed09215806dca9ff504d8c0dda5da68d7f2c67d34a231d82\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.28.0-cp37-none-any.whl size=15559 sha256=9f6831ab137b191c07ecc2847b22c70c5f0634607f0509ca0b8fbfdc553a5fe9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/92/bd/b3407bc29501f7e28eb970a6c425a9a375485c5d8197df6a8f\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.15.0-cp37-none-any.whl size=5224 sha256=56a17021374c038e02f2f2f6dbc5fc75747fddbe11cb7f8e0aec89a90329e156\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/ea/db/833c8a9b8326e703e9f8a78c0d4153294e6a1b1f97a1836397\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.7.0-cp37-none-any.whl size=5542 sha256=d8ca010d940151d4e68d54bb1d779e70e0e416ecfaea522aa3e724c75c3e9b5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/d2/f6/58ce0aae0055dbccba8b40e62a6c22ab997105ad8c431a9e80\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.9.0-cp37-none-any.whl size=4505 sha256=abf78ae9f3f1eef176d71d7ca7943953c7be2dbddb21529c4b459c14ac5fa91d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/d5/7d/bef5ee93c88bc6150294cc74cbb081647c505bf816918dd7ff\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-cp37-none-any.whl size=19416 sha256=d96d977f0eb116dfa818caa240ce1cba588262a0bc741b12125ba186105285ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/93/0d/891573db60f74d0e43bd7db1496c3ef898f8b5946a4c24cbda\n",
            "Successfully built keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer\n",
            "Successfully installed keras-embed-sim-0.9.0 keras-layer-normalization-0.15.0 keras-multi-head-0.28.0 keras-pos-embd-0.12.0 keras-position-wise-feed-forward-0.7.0 keras-self-attention-0.50.0 keras-transformer-0.39.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpALIVhda2Uq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn0653pSkx7w"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1'\n",
        "from keras_transformer import get_model, decode\n",
        "from pickle import load\n",
        "from google.colab import drive\n",
        "np.random.seed(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfyDMRNE2li5"
      },
      "source": [
        "#os.listdir('/drive/MyDrive/Colab Notebooks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz72Kbs7lLs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac26b067-0fc6-4596-cbd7-0e9608241ea6"
      },
      "source": [
        "# Leer set de entrenamiento\n",
        "drive.mount('/content/drive')\n",
        "filename = '/content/drive/My Drive/Colab Notebooks/dataset.npy'\n",
        "\n",
        "dataset = np.load(filename)\n",
        "print(dataset[120000,0])\n",
        "print(dataset[120000,1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "litre\n",
            "litro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUkj6RHgf8rJ",
        "outputId": "d438d852-ea37-4814-816c-8819fd63d442"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1Vu8tRqlmzl",
        "outputId": "367317cc-8bb3-4f81-b346-4edf1b0a60cc"
      },
      "source": [
        "# Crear \"tokens\"\n",
        "source_tokens = []\n",
        "for sentence in dataset[:,0]:\n",
        "  source_tokens.append(sentence.split(' '))\n",
        "print(source_tokens[120000])\n",
        "\n",
        "target_tokens = []\n",
        "for sentence in dataset[:,1]:\n",
        "  target_tokens.append(sentence.split(' '))\n",
        "print(target_tokens[120000])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['litre']\n",
            "['litro']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-asqUiVmHH-"
      },
      "source": [
        "def build_token_dict(token_list):\n",
        "  token_dict = {\n",
        "      '<PAD>': 0,\n",
        "      '<START>': 1,\n",
        "      '<END>': 2\n",
        "  }\n",
        "  for tokens in token_list:\n",
        "    for token in tokens:\n",
        "      if token not in token_dict:\n",
        "        token_dict[token] = len(token_dict)\n",
        "  return token_dict"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K176ljam8fu",
        "outputId": "b820ce42-3c10-4d56-97ed-f375beff16fe"
      },
      "source": [
        "source_token_dict = build_token_dict(source_tokens)\n",
        "target_token_dict = build_token_dict(target_tokens)\n",
        "target_token_dict_inv = {v:k for k,v in target_token_dict.items()}\n",
        "\n",
        "print(len(source_token_dict))\n",
        "print(len(target_token_dict))\n",
        "print(len(target_token_dict_inv))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95532\n",
            "110263\n",
            "110263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRxmhL-Xer7c",
        "outputId": "10fea2b4-4af0-48d8-84db-61fccd680bc8"
      },
      "source": [
        "len(source_token_dict)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95532"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgTBWTgwncYr"
      },
      "source": [
        "# Agregar start, end y pad a cada frase del set de entrenamiento\n",
        "encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
        "decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
        "output_tokens = [tokens + ['<END>'] for tokens in target_tokens]\n",
        "\n",
        "source_max_len = max(map(len, encoder_tokens))\n",
        "target_max_len = max(map(len, decoder_tokens))\n",
        "\n",
        "encoder_tokens = [tokens + ['<PAD>']*(source_max_len-len(tokens)) for tokens in encoder_tokens]\n",
        "decoder_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in decoder_tokens]\n",
        "output_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in output_tokens ]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYXvsgPVpGzS",
        "outputId": "f679e831-a0be-4363-fba2-e53f9f62848f"
      },
      "source": [
        "print(encoder_tokens[120000])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<START>', 'litre', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9okm2wYpNPW",
        "outputId": "60ab5ed8-7e8d-4a5c-87df-aaf876e51e95"
      },
      "source": [
        "encoder_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encoder_tokens]\n",
        "decoder_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decoder_tokens]\n",
        "output_decoded = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]\n",
        "\n",
        "print(encoder_input[120000])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 53982, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFHSR9y3p0D3",
        "outputId": "75ab2147-e9f9-49a9-fbdc-c2cb8afe011d"
      },
      "source": [
        "# Crear la red transformer\n",
        "model = get_model(\n",
        "    token_num = max(len(source_token_dict),len(target_token_dict)),\n",
        "    embed_dim = 32,\n",
        "    encoder_num = 2,\n",
        "    decoder_num = 2,\n",
        "    head_num = 2,\n",
        "    hidden_dim = 128,\n",
        "    dropout_rate = 0.05,\n",
        "    use_same_embed = False,\n",
        ")\n",
        "model.compile('adam', 'sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Encoder-Input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Token-Embedding (Embedd [(None, None, 32), ( 3528416     Encoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Embedding (TrigPosEmbed (None, None, 32)     0           Encoder-Token-Embedding[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     4224        Encoder-Embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-Embedding[0][0]          \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     64          Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, None, 32)     8352        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, None, 32)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, None, 32)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, None, 32)     64          Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     4224        Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Token-Embedding (Embedd [(None, None, 32), ( 3528416     Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Embedding (TrigPosEmbed (None, None, 32)     0           Decoder-Token-Embedding[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     64          Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     4224        Decoder-Embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, None, 32)     8352        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, None, 32)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-Embedding[0][0]          \n",
            "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, None, 32)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     64          Decoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, None, 32)     64          Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     4224        Decoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     64          Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward (FeedForw (None, None, 32)     8352        Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Dropout ( (None, None, 32)     0           Decoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Add (Add) (None, None, 32)     0           Decoder-1-MultiHeadQueryAttention\n",
            "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Norm (Lay (None, None, 32)     64          Decoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     4224        Decoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     64          Decoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     4224        Decoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     64          Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward (FeedForw (None, None, 32)     8352        Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Dropout ( (None, None, 32)     0           Decoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Add (Add) (None, None, 32)     0           Decoder-2-MultiHeadQueryAttention\n",
            "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Norm (Lay (None, None, 32)     64          Decoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Output (EmbeddingSim)   (None, None, 110263) 110263      Decoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-Token-Embedding[0][1]    \n",
            "==================================================================================================\n",
            "Total params: 7,226,487\n",
            "Trainable params: 7,226,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynMwnUEIbclM"
      },
      "source": [
        "#from keras.callbacks import ModelCheckpoint\n",
        "#from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDxxQTKTrX6x",
        "outputId": "54707e21-7959-4037-dbd3-d38b2e52cc72"
      },
      "source": [
        "#    Entrenamiento\n",
        "#x = np.hstack((np.array(encoder_input), np.array(decoder_input)))\n",
        "#y = np.array(output_decoded)\n",
        "#x_trainNP,x_testNP,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
        "#x_train=[x_trainNP[:,0:49],x_trainNP[:,49:]]\n",
        "#X_test=[x_testNP[:,0:49],x_testNP[:,49:]]\n",
        "x = [np.array(encoder_input), np.array(decoder_input)]\n",
        "y = np.array(output_decoded)\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/tp'\n",
        "output_path = os.path.join(DATA_PATH,'output2')\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "filename = os.path.join(output_path)\n",
        "#checkpoint = ModelCheckpoint(filepath=filename, monitor='loss', verbose=1,\n",
        "#    save_best_only=True, mode='auto', save_freq=1)\n",
        "\n",
        "history=model.fit(x,y, epochs=15, batch_size=50,validation_split=0.1)\n",
        "\n",
        "#filename = '/content/drive/My Drive/videos/2020-07-06/translator.h5'\n",
        "#model.load_weights(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "4259/4259 [==============================] - 794s 186ms/step - loss: 0.4523 - val_loss: 0.4643\n",
            "Epoch 2/15\n",
            "4259/4259 [==============================] - 794s 186ms/step - loss: 0.3291 - val_loss: 0.3813\n",
            "Epoch 3/15\n",
            "4259/4259 [==============================] - 795s 187ms/step - loss: 0.2754 - val_loss: 0.3532\n",
            "Epoch 4/15\n",
            "1257/4259 [=======>......................] - ETA: 8:51 - loss: 0.2398"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vraIQBpSO--c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW3AuUrBsAX7"
      },
      "source": [
        "def translate(sentence):\n",
        "  sentence_tokens = [tokens + ['<END>', '<PAD>'] for tokens in [sentence.split(' ')]]\n",
        "  tr_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in sentence_tokens][0]\n",
        "  decoded = decode(\n",
        "      model, \n",
        "      tr_input, \n",
        "      start_token = target_token_dict['<START>'],\n",
        "      end_token = target_token_dict['<END>'],\n",
        "      pad_token = target_token_dict['<PAD>']\n",
        "  )\n",
        "\n",
        "  print('Frase original: {}'.format(sentence))\n",
        "  print('Traducción: {}'.format(' '.join(map(lambda x: target_token_dict_inv[x], decoded[1:-1]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5kitzaitgUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae9b9ec-cfca-4f8b-f76f-c25b3c2917b3"
      },
      "source": [
        "translate('the day is warm and sunny')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frase original: the day is warm and sunny\n",
            "Traducción: el dia es caliente y sol\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4VrVBcntlVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8b77f0-17c9-4bc8-fa4c-abd51dae5cc9"
      },
      "source": [
        "translate('i am tired of college')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frase original: i am tired of college\n",
            "Traducción: estoy una sola\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiBYdZQBD-41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edd26cb-f4a0-4ac1-d1c9-a47d115527f8"
      },
      "source": [
        "a='why this net doesnt work Right'\n",
        "a=a.lower()\n",
        "translate(a)\n",
        "model.save(\"2heads_50batch_trueshuffle_dataset1.9\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frase original: why this net doesnt work right\n",
            "Traducción: por que la vez no el trabajo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) Encoder-Input, Decoder-Input with unsupported characters which will be renamed to encoder_input, decoder_input in the SavedModel.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: OP/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: OP/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "migfbHtbBdPY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}